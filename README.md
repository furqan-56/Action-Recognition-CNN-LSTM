# ğŸ¬ Human Activity Recognition System

> **An intelligent deep learning solution for video-based human action classification using hybrid CNN-LSTM architecture**

[![Python Version](https://img.shields.io/badge/python-3.8%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.10%2B-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org)
[![Flask](https://img.shields.io/badge/Flask-API-000000?style=for-the-badge&logo=flask&logoColor=white)](https://flask.palletsprojects.com)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Model Architecture](#-model-architecture)
- [Dataset Information](#-dataset-information)
- [Performance Metrics](#-performance-metrics)
- [Quick Start Guide](#-quick-start-guide)
- [API Documentation](#-api-documentation)
- [Project Layout](#-project-layout)
- [Technical Implementation](#-technical-implementation)
- [Future Enhancements](#-future-enhancements)

---

## ğŸ¯ Overview

This **Human Activity Recognition System** is a sophisticated deep learning solution that leverages convolutional neural networks combined with LSTM recurrent architectures to understand and classify human activities in video sequences. The system processes temporal information across multiple frames, enabling accurate recognition of complex motion patterns and human actions.

### Why This Approach?

Traditional image classifiers analyze single frames independently, missing crucial temporal dynamics. Our hybrid architecture addresses this limitation:

```
Single Frame Analysis â†’ Limited context, no motion understanding
Sequence Analysis     â†’ Full temporal context, motion patterns captured
```

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ§  **Hybrid Architecture** | Combines spatial feature extraction with temporal sequence modeling |
| ğŸš€ **Real-time Processing** | Optimized inference pipeline for quick predictions |
| ğŸŒ **REST API** | Clean, well-documented endpoints for easy integration |
| ğŸ¨ **Modern Web Interface** | Intuitive drag-and-drop video upload with live preview |
| ğŸ“Š **Detailed Analytics** | Probability distributions across all activity classes |
| ğŸ“± **Responsive Design** | Works seamlessly on desktop and mobile devices |

---

## ğŸ—ï¸ Model Architecture

Our recognition pipeline employs a two-stage feature extraction approach:

### Architectural Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT LAYER                                  â”‚
â”‚              (20 frames Ã— 224 Ã— 224 Ã— 3)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SPATIAL FEATURE EXTRACTOR                          â”‚
â”‚         TimeDistributed(MobileNetV2)                            â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚    â€¢ Pre-trained on ImageNet (1000 classes)                    â”‚
â”‚    â€¢ Global Average Pooling output                             â”‚
â”‚    â€¢ Processes each frame independently                         â”‚
â”‚    â€¢ Output: 20 Ã— 1280 feature vectors                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TEMPORAL SEQUENCE LEARNER                          â”‚
â”‚                    LSTM (64 units)                              â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚    â€¢ Captures motion dynamics across frames                     â”‚
â”‚    â€¢ Learns temporal dependencies                               â”‚
â”‚    â€¢ Output: 64-dimensional context vector                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              REGULARIZATION LAYER                               â”‚
â”‚                  Dropout (p=0.5)                                â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚    â€¢ Prevents overfitting during training                       â”‚
â”‚    â€¢ Disabled during inference                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CLASSIFICATION HEAD                                â”‚
â”‚              Dense(11, softmax)                                 â”‚
â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚    â€¢ Multi-class probability distribution                       â”‚
â”‚    â€¢ 11 activity categories                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

| Component | Specification | Purpose |
|-----------|---------------|---------|
| **Backbone CNN** | MobileNetV2 (ImageNet weights) | Efficient spatial feature extraction |
| **Temporal Module** | LSTM with 64 hidden units | Learning frame-to-frame relationships |
| **Dropout Rate** | 50% | Regularization to improve generalization |
| **Activation** | Softmax | Probability distribution over classes |

---

## ğŸ“ Dataset Information

The model is trained and evaluated on the **UCF11 YouTube Actions** dataset, a benchmark for human action recognition research.

### Supported Activity Categories

| Category | Description | Example Motions |
|----------|-------------|-----------------|
| ğŸ€ **Basketball** | Basketball shooting | Dribbling, shooting, jumping |
| ğŸš´ **Biking** | Bicycle riding | Pedaling, steering |
| ğŸŠ **Diving** | Platform/springboard diving | Approach, takeoff, rotation |
| â›³ **Golf Swing** | Golf club swinging | Backswing, downswing, follow-through |
| ğŸ´ **Horse Riding** | Equestrian activities | Trotting, galloping, jumping |
| âš½ **Soccer Juggling** | Keeping ball airborne | Foot, knee, head touches |
| ğŸª **Swing** | Playground swinging | Forward-backward oscillation |
| ğŸ¾ **Tennis Swing** | Tennis strokes | Forehand, backhand, serve |
| ğŸ¤¸ **Trampoline Jumping** | Trampoline bouncing | Various aerial maneuvers |
| ğŸ **Volleyball Spiking** | Volleyball attack | Approach, jump, spike |
| ğŸš¶ **Walking** | Human locomotion | Forward walking motion |

---

## ğŸ“ˆ Performance Metrics

### Training Progress Visualization

![Training Metrics](plots/accuracy_loss.png)

*Figure 1: Training and validation accuracy/loss curves over epochs*

### Classification Performance

![Confusion Matrix](plots/confusion_matrix.png)

*Figure 2: Confusion matrix showing per-class prediction accuracy*

---

## ğŸš€ Quick Start Guide

### Prerequisites

Ensure you have the following installed:
- Python 3.8 or higher
- pip package manager
- FFmpeg (for video processing)

### Step 1: Clone & Navigate

```bash
# Clone the repository
git clone <repository-url>
cd DLAssignment
```

### Step 2: Install Dependencies

```bash
# Navigate to backend directory
cd backend

# Install required packages
pip install -r requirements.txt
```

### Step 3: Launch the Server

```bash
# Start the API server
python app.py
```

You should see:
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      Human Activity Recognition - REST API Server           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Status     : Active                                         â•‘
â•‘  Endpoint   : http://127.0.0.1:5000                         â•‘
â•‘  Categories : 11 activity classes                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Step 4: Open the Interface

Open `frontend/index.html` in your web browser to access the interactive interface.

---

## ğŸ“¡ API Documentation

### Base URL
```
http://127.0.0.1:5000
```

### Endpoints

#### Health Check
```http
GET /
```
Returns server status and available endpoints.

**Response:**
```json
{
    "status": "operational",
    "service": "Human Activity Recognition API",
    "model_info": {
        "architecture": "CNN-LSTM Hybrid",
        "categories": 11
    }
}
```

#### Get Activity Categories
```http
GET /categories
```
Returns list of recognizable activities.

**Response:**
```json
{
    "categories": ["basketball", "biking", ...],
    "total": 11
}
```

#### Analyze Video
```http
POST /predict
Content-Type: multipart/form-data
```

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `video` | file | Yes | Video file (MP4, AVI, MOV, etc.) |

**Success Response (200):**
```json
{
    "success": true,
    "prediction": {
        "activity": "basketball",
        "confidence": 94.67,
        "category_index": 0
    },
    "probability_distribution": {
        "basketball": 0.9467,
        "biking": 0.0234,
        ...
    }
}
```

**Error Response (400):**
```json
{
    "success": false,
    "error": "invalid_format",
    "message": "Unsupported video format. Please use MP4, AVI, or MOV."
}
```

---

## ğŸ“‚ Project Layout

```
DLAssignment/
â”‚
â”œâ”€â”€ ğŸ“ backend/                    # Server-side components
â”‚   â”œâ”€â”€ app.py                     # Flask REST API application
â”‚   â”œâ”€â”€ model_loader.py            # Neural network initialization
â”‚   â”œâ”€â”€ video_utils.py             # Video preprocessing pipeline
â”‚   â””â”€â”€ requirements.txt           # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“ frontend/                   # Client-side interface
â”‚   â”œâ”€â”€ index.html                 # Main application page
â”‚   â”œâ”€â”€ style.css                  # Visual styling
â”‚   â””â”€â”€ script.js                  # Interactive functionality
â”‚
â”œâ”€â”€ ğŸ“ models/                     # Trained model artifacts
â”‚   â”œâ”€â”€ ucf11_cnn_lstm_model.h5   # Serialized model weights
â”‚   â””â”€â”€ classes.json               # Category label mapping
â”‚
â”œâ”€â”€ ğŸ“ plots/                      # Training visualizations
â”‚   â”œâ”€â”€ accuracy_loss.png          # Learning curves
â”‚   â””â”€â”€ confusion_matrix.png       # Classification report
â”‚
â””â”€â”€ ğŸ“„ README.md                   # Project documentation
```

---

## ğŸ”§ Technical Implementation

### Video Processing Pipeline

```python
# Preprocessing flow for each video
Video File
    â”‚
    â”œâ”€â”€â–º Frame Extraction (20 frames, uniform sampling)
    â”‚
    â”œâ”€â”€â–º Spatial Resizing (224 Ã— 224 pixels)
    â”‚
    â”œâ”€â”€â–º Color Normalization ([0, 255] â†’ [0, 1])
    â”‚
    â”œâ”€â”€â–º Batch Dimension Addition
    â”‚
    â””â”€â”€â–º Model Input: (1, 20, 224, 224, 3)
```

### Key Design Decisions

1. **MobileNetV2 Backbone**: Chosen for its excellent accuracy-to-computation ratio, enabling faster inference without significant accuracy loss.

2. **Fixed Sequence Length**: 20 frames provide sufficient temporal context while maintaining computational efficiency.

3. **Transfer Learning**: ImageNet pre-training provides robust low-level feature extractors, reducing training data requirements.

4. **LSTM over GRU**: Selected for its superior performance on longer sequences due to separate memory cell architecture.

---

## ğŸ”® Future Enhancements

- [ ] **Attention Mechanisms**: Implement temporal attention for focusing on key frames
- [ ] **Multi-Scale Processing**: Process videos at multiple temporal resolutions
- [ ] **Real-Time Streaming**: Enable webcam input for live activity recognition
- [ ] **Model Optimization**: TensorFlow Lite conversion for mobile deployment
- [ ] **Extended Dataset**: Training on UCF101 for broader activity coverage

---

## ğŸ“„ License

This project is developed for educational purposes as part of a Deep Learning course assignment.

---

<div align="center">

**Built with â¤ï¸ using TensorFlow & Flask**

*Deep Learning Assignment - Human Activity Recognition using CNN-LSTM*

</div>

## Sample Test Videos

Download sample videos from UCF11 dataset to test:
- [UCF11 Dataset](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php)

Or use any short video (MP4/AVI) showing these actions:
- Basketball, Biking, Diving, Golf, Horse Riding
- Soccer Juggling, Swing, Tennis, Trampoline, Volleyball, Walking

## Tech Stack

- TensorFlow/Keras
- MobileNetV2 (ImageNet pretrained)
- Flask REST API
- ImageIO, OpenCV

## Dataset

[UCF11 - YouTube Action Dataset](https://www.crcv.ucf.edu/data/UCF_YouTube_Action.php)


